{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Dive-into-DL-Pytorch-4.5-读取和存储.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakurasakura1996/Pytorch-start-learning/blob/master/Dive_into_DL_Pytorch_4_5_%E8%AF%BB%E5%8F%96%E5%92%8C%E5%AD%98%E5%82%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ_AxPinjq_a",
        "colab_type": "text"
      },
      "source": [
        "# 4.5读取和存储\n",
        "使用save load 分别存储和读取Tensor:\n",
        "    save 使用Python的pickle实用程序将对象进行序列化，然后将序列化的对象保存到disk中，使用save可以保存各种对象，包括模型、张量和字典等\n",
        "    load使用pickle unpickle工具将pickle的对象文件反序列化为内存\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVVe9p3Wjq_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "x = torch.ones(3)\n",
        "torch.save(x, 'x.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odIehG4Bjq_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f20e64d7-4dc1-4bb4-8298-c2a5e2d66fcb"
      },
      "source": [
        "# 然后再从数据中将其读入内存\n",
        "x2 = torch.load('x.pt')\n",
        "print(x2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGXGxWiujq_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8624f560-86d0-4aa8-a850-cc6b0c660f69"
      },
      "source": [
        "# 还可以存储一个Tensor列表并读回内存\n",
        "y = torch.zeros(4)\n",
        "torch.save([x,y],'xy.pt')\n",
        "\n",
        "xy_list_load = torch.load('xy.pt')\n",
        "print(type(xy_list_load))\n",
        "print(len(xy_list_load))   # 一个tensor视为一个列表元素\n",
        "print(xy_list_load)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "2\n",
            "[tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjb1GABrjq_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b18b6348-1fd9-4b7c-de7c-a6b8e34ba508"
      },
      "source": [
        "# 当然 存储并读取 从一个字符串映射到 Tensor的字典\n",
        "torch.save({\n",
        "    'x':x,\n",
        "    'y':y\n",
        "},'xy_dict.pt')\n",
        "\n",
        "xy_dict_load = torch.load('xy_dict.pt')\n",
        "for key, value in xy_dict_load.items():\n",
        "    print(key,value)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x tensor([1., 1., 1.])\n",
            "y tensor([0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMEJeFJmjq_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "065acf79-c077-4f8b-9417-7b8aa1d0207b"
      },
      "source": [
        "# 读写模型  state_dict\n",
        "# 在pytorch中，Module的可学习参数（即权重和偏差），模块模型包含在参数中（通过model.parameters()访问）。state_dict是一个从参数名称映射到参数Tensor的字典对象\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()\n",
        "        self.hidden = nn.Linear(3, 2)   # 注意这里没有逗号啊，加了逗号发现后面state_dict()没有隐藏层的参数了\n",
        "        self.act = nn.ReLU()\n",
        "        self.output = nn.Linear(2, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        a = self.act(self.hidden(x))\n",
        "        return self.output(a)\n",
        "    \n",
        "net = MLP()\n",
        "print(net.state_dict())   # OrderedDict类型\n",
        "print(net.parameters())  # 打印出来都是一个 generator 迭代器\n",
        "print(net.named_parameters())\n",
        "\n",
        "print('-----------------------')\n",
        "for param in net.parameters():\n",
        "    print(param)\n",
        "    print(param.data)\n",
        "    \n",
        "print('-----------------------')\n",
        "for name, param in net.named_parameters():\n",
        "    print(name, param.data)\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('hidden.weight', tensor([[ 0.0757, -0.0958, -0.1102],\n",
            "        [ 0.4308,  0.2259,  0.5228]])), ('hidden.bias', tensor([ 0.3304, -0.4504])), ('output.weight', tensor([[ 0.3593, -0.5808]])), ('output.bias', tensor([0.6307]))])\n",
            "<generator object Module.parameters at 0x7fbea4069db0>\n",
            "<generator object Module.named_parameters at 0x7fbea4069db0>\n",
            "-----------------------\n",
            "Parameter containing:\n",
            "tensor([[ 0.0757, -0.0958, -0.1102],\n",
            "        [ 0.4308,  0.2259,  0.5228]], requires_grad=True)\n",
            "tensor([[ 0.0757, -0.0958, -0.1102],\n",
            "        [ 0.4308,  0.2259,  0.5228]])\n",
            "Parameter containing:\n",
            "tensor([ 0.3304, -0.4504], requires_grad=True)\n",
            "tensor([ 0.3304, -0.4504])\n",
            "Parameter containing:\n",
            "tensor([[ 0.3593, -0.5808]], requires_grad=True)\n",
            "tensor([[ 0.3593, -0.5808]])\n",
            "Parameter containing:\n",
            "tensor([0.6307], requires_grad=True)\n",
            "tensor([0.6307])\n",
            "-----------------------\n",
            "hidden.weight tensor([[ 0.0757, -0.0958, -0.1102],\n",
            "        [ 0.4308,  0.2259,  0.5228]])\n",
            "hidden.bias tensor([ 0.3304, -0.4504])\n",
            "output.weight tensor([[ 0.3593, -0.5808]])\n",
            "output.bias tensor([0.6307])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD3H62oWjq_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "5628135b-d5b5-4bf8-9825-2057c0d8b0d0"
      },
      "source": [
        "# 注意，只有具有可学习参数的层（卷积层、线性层等）才有state_dict中的条目。优化器optim 也有一个state_dict,其中包含关于优化器状态以及所使用的超参数的信息\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer.state_dict()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'param_groups': [{'dampening': 0,\n",
              "   'lr': 0.001,\n",
              "   'momentum': 0.9,\n",
              "   'nesterov': False,\n",
              "   'params': [140455476532280,\n",
              "    140455476532208,\n",
              "    140455476532352,\n",
              "    140455476532424],\n",
              "   'weight_decay': 0}],\n",
              " 'state': {}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIO4hxb1jq__",
        "colab_type": "text"
      },
      "source": [
        "保存和加载模型  pytorch中保存和加载训练模型有两种常见的方法：\n",
        "1.仅保存和加载模型参数（state_dict);   \n",
        "2.保存和加载整个模型\n",
        "    1.torch.save(model.state_dict(), PATH)   建议文件后缀名是 pt 或者 pth\n",
        "      加载： model = TheModelClass(*args, **kwargs)   model.load_state_dict(torch.load(PATH))\n",
        "    2.torch.save(model,PATH)\n",
        "      加载： model = torch.load(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGFV8wbjrAB",
        "colab_type": "text"
      },
      "source": [
        "# 4.6GPU计算\n",
        "以上都还是再用cpu再跑程序，这一节介绍用 GPU来计算。由于我这笔记本垃圾的不行，看来这一节的学习就搬到 google colab上去学习啦，不过还是在这里写一遍吧，书上没太多内容"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw7_iqB_kPCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "146dc050-848f-495e-80cb-23b64b52aa9a"
      },
      "source": [
        "!nvidia-smi\n",
        "# 查看显卡信息，这里是 Tesla P100  16GB显存，卧槽还可以啊"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar  3 13:30:48 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    29W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "2YSsQhKcjrAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a9e39c42-4fc6-45ac-e1f4-9db39a1adeff"
      },
      "source": [
        "# pytorch可以指定用来存储和计算的设备，如使用内存的CPU  或者使用显存的GPU。默认情况下，pytorch会将数据创建再内存，然后用CPU计算\n",
        "# 用torch.cuda.is_available()\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())  # 查看GPU数量\n",
        "print(torch.cuda.current_device())   # 查看当前GPU 索引号，索引号从0开始，由于这里没有所有会报错啦  呜呜呜\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n",
            "0\n",
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgWV0Q21jrAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a4d82ff-db81-46f5-cb2b-bdfe5b04ed59"
      },
      "source": [
        "# 默认tensor会被存在内存上。因此之前打印tensor信息时看不到GPU相关标识\n",
        "x =torch.tensor([1,2,3])\n",
        "print(x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNQZOanimIW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "77bec7b1-bbdc-4344-be3d-dc9a04828ee5"
      },
      "source": [
        "# 使用 .cuda() 可以将 CPU 上的tensor转换复制到 GPU上，如果有多快卡，可以通过cuda(i)来表示第i块GPU及相应的显存 （i从0开始） 且cuda(0) 和 cuda()相同\n",
        "x = x.cuda(0)\n",
        "print(x)\n",
        "# 通过tensor的device属性来查看该 Tensor所在的设备\n",
        "print(x.device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3], device='cuda:0')\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnB12codmihJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "08ec0a04-b390-4dc5-aba1-1c3c8ea85db7"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "x = torch.tensor([1, 2, 3], device=device)\n",
        "print(x)\n",
        "# 下面是另一种方式\n",
        "x = torch.tensor([1, 2, 3]).to(device)\n",
        "print(x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3], device='cuda:0')\n",
            "tensor([1, 2, 3], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDj6bYXhnJdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85ea8d87-5809-4a29-be35-09d5ce9833e7"
      },
      "source": [
        "# 如果对在 GPU 上的数据进行运算，那么结果还是存放在 GPu上的\n",
        "y = x**2\n",
        "print(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 4, 9], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iceSuc6hnaHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 需要注意的是，存储在不同位置中的数据是不可以直接进行运算的。及存放在cpu上的数据不可以直接与存放在GPU上的数据进行运算，位于不同GPU上的数据也是不能直接进行运算的\n",
        "z = y + x.cpu()  # 这里就会报错了"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb3ZU7QonuDE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbbf012b-74d8-4751-ada8-2fc22445ca23"
      },
      "source": [
        "# 4.6.3 模型的GPU计算\n",
        "# 同 Tensor类似，Pytorch模型也可以通过 .cuda 转换到GPU上。我们可以通过检查模型的参数的device属性来查看存放模型的设备\n",
        "net = nn.Linear(3, 1)\n",
        "list(net.parameters())[0].device"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_3rfwP8oZFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdda395d-3f46-4f73-ced1-74b7771a93a2"
      },
      "source": [
        "net = net.cuda()\n",
        "list(net.parameters())[0].device"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_45lF4soek-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be29363e-2531-4eb8-dcb6-c94625871d9a"
      },
      "source": [
        "# 同样的我们要保证 参与到模型运算的tensor都要在 GPU显存上，\n",
        "x = torch.rand(2,3).cuda()\n",
        "net(x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1951],\n",
              "        [-0.0600]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8AZyZqosVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}