{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dive-into-DL-Pytorch-2.2数据操作.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Q2SbhrsZ_PfE11vk8-P3ImgsydjEliWs",
      "authorship_tag": "ABX9TyMNIP8MYKSh/puEqzeT233l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakurasakura1996/Pytorch-start-learning/blob/master/Dive_into_DL_Pytorch_2_2%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttLIzNnj8JLB",
        "colab_type": "code",
        "outputId": "df260218-0240-4166-9772-498e5e1a95f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vYRR1ulwjO5",
        "colab_type": "code",
        "outputId": "eddef3e6-514e-4227-9093-b13ba96e0c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6TVE95zwEqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"./drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqIQHuNfPAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"./My Drive/pytorch_learning\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q156qC8Uf7wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据操作 torch.Tensor 是存储和变换数据的主要工具，用过numpy的话会觉的Tensor和numpy中的多维数组很相似。然而，Tensor提供了GPU运算和自动梯度\n",
        "# 等更多功能。所以Tensor更适合深度学习 \"tensor\"这个单词一般翻译为张量，张量可以看成多维数组，标量是0维张量，向量是一维张量，矩阵是二维张量\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7_MivnRhWpr",
        "colab_type": "code",
        "outputId": "56074e84-b4c5-44ec-ca41-71ff8213aa90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# 2.2.1 创建Tensor\n",
        "x = torch.empty(5, 3)\n",
        "print(x)\n",
        "# 这里创建的是未初始化的tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[5.2157e-38, 0.0000e+00, 4.4842e-44],\n",
            "        [0.0000e+00,        nan, 0.0000e+00],\n",
            "        [3.3356e-09, 6.6474e+22, 8.3193e+20],\n",
            "        [2.1005e+20, 1.6822e-04, 8.5381e-07],\n",
            "        [2.6687e-09, 3.2917e-09, 5.0948e-14]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBSdwC7ghffS",
        "colab_type": "code",
        "outputId": "2f1cc6a6-7187-45ba-af3f-d1908f021f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "x = torch.rand(5,3)\n",
        "print(x)\n",
        "# 创建了随机初始化的tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7065, 0.8768, 0.6812],\n",
            "        [0.6976, 0.3529, 0.4352],\n",
            "        [0.3454, 0.3821, 0.0897],\n",
            "        [0.9557, 0.7654, 0.7216],\n",
            "        [0.6035, 0.3757, 0.8977]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2sQNF-uheZm",
        "colab_type": "code",
        "outputId": "d3507bdc-c552-4691-e71e-9bf980a0a60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# 创建long型全0的tensor，注意类型的定义,用dtype\n",
        "x = torch.zeros(5,3,dtype=torch.long)\n",
        "print(x)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqGmoMRbiOOi",
        "colab_type": "code",
        "outputId": "63af5629-1b58-4b19-add6-d85099f7d77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 还可以直接根据数据来创建\n",
        "x =torch.tensor([4, 3])\n",
        "print(x)\n",
        "print(type(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4, 3])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqXOA5h0ioZK",
        "colab_type": "code",
        "outputId": "98d02805-04bc-41fc-8c1f-80b05a5448be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# 还可以通过现有的Tensor创建，此方法会默认重用输入 Tensor的一些属性，例如数据类型，除非自定义数据类型\n",
        "x = x.new_ones(4, 4)  # 返回的tensor默认具有相同torch.dtype 和 torch.device（cpu or gpu)\n",
        "# 上面的意思就是新创建的tensor和已有的x tensor拥有相同的dtype和device 除非特殊声明，new_ones()中的size参数不能少\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "\n",
        "x = torch.rand_like(x, dtype=torch.float)   # 指定数据类型\n",
        "print(x)\n",
        "print(x.dtype)    # torch.float默认是 float32"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 1, 1, 1],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 1, 1, 1]])\n",
            "torch.int64\n",
            "tensor([[0.7826, 0.0348, 0.4266, 0.4664],\n",
            "        [0.4528, 0.8582, 0.5283, 0.2574],\n",
            "        [0.8035, 0.4462, 0.6204, 0.5401],\n",
            "        [0.8533, 0.5525, 0.7462, 0.3785]])\n",
            "torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU4BZtNekiK2",
        "colab_type": "code",
        "outputId": "29485087-0168-4bed-b456-9a39505bf6ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# 可以通过shape 或者 size() 来获取tensor 的形状\n",
        "print(x.size())\n",
        "print(x.shape)\n",
        "# 注意，返回的torch.Size 其实就是一个 tuple， 支持所有tuple的操作 元组\n",
        "print(type(x.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4])\n",
            "torch.Size([4, 4])\n",
            "<class 'torch.Size'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PnM8EDOkiHy",
        "colab_type": "code",
        "outputId": "c290f232-6d6c-453d-9a23-5dad961b6e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# 还有很多种创建tensor的方式，这里再列举 均匀和标准分布的例子，\n",
        "x = torch.rand(5,3)   # 随机初始化 就是均匀分布\n",
        "print(x)\n",
        "x = torch.randn(5,3)   # 标准正态分布\n",
        "print(x)\n",
        "\n",
        "# x = torch.normal(0,0.5)   # 正态分布，参数中分别是均值mean和标准差 std 这里写的还不多，normal() 要求mean 和 std必须是Tensor 和 float中的\n",
        "# 一种，两两组合就有四种情况都可以定义出来，并且后需要定义参数 torch.Generator  和 Tensor out两个参数，\n",
        "x = torch.normal(mean=0.5, std=torch.arange(1.0,6))   # 离散正态分布 返回一个张量，包含了从指定均值means和标准差std的离散正态分布中抽取的一组随机数。\n",
        "# 标准差std是一个张量，包含每个输出元素相关的正态分布标准差。\n",
        "print(x)   # 这里要注意 torch.arange(1,6)会返回一个longType的形式而报错，"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7441, 0.1626, 0.3877],\n",
            "        [0.0969, 0.4559, 0.3553],\n",
            "        [0.0619, 0.2029, 0.2093],\n",
            "        [0.1654, 0.8216, 0.2568],\n",
            "        [0.1894, 0.5788, 0.5157]])\n",
            "tensor([[ 0.7054,  0.7837,  0.3517],\n",
            "        [-0.4680, -0.9191,  0.3504],\n",
            "        [-3.2290, -0.5712, -0.5749],\n",
            "        [-0.1826,  0.2466,  1.7595],\n",
            "        [-1.1130, -1.6568, -1.3066]])\n",
            "tensor([ 0.2486,  2.4009, -3.9355,  0.2599, -0.7448])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDE1-FkqkiFK",
        "colab_type": "code",
        "outputId": "f906ca01-76a5-4001-c516-98e429668f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# 创建tensor时都可以指定数据类型和存放device（cpu or gpu)\n",
        "x = torch.randn(5,3,dtype=torch.float16,device='cuda')\n",
        "print(x)\n",
        "print(x.device)\n",
        "\n",
        "y = torch.randn(5,3)\n",
        "print(y.device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1692, -1.4209,  1.4619],\n",
            "        [-0.1663,  0.1781,  1.1670],\n",
            "        [-0.1111, -0.0269,  0.0755],\n",
            "        [ 0.8491, -0.0025,  0.0340],\n",
            "        [ 0.1139,  0.5752, -0.1713]], device='cuda:0', dtype=torch.float16)\n",
            "cuda:0\n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEO-FtoFkiCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.2.2 操作\n",
        "# 介绍tensor各种操作"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjZ-9GSVkh2U",
        "colab_type": "code",
        "outputId": "7df7421e-68ea-4d85-ad18-80ca4a66ab9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# 算术操作   pytorch中，同一种操作可能有很多形式\n",
        "# 加法  \n",
        "# 形式一\n",
        "x = torch.rand(5,3)\n",
        "print(x+y)\n",
        "# 形式二\n",
        "print(torch.add(x,y))\n",
        "# 形式三 指定输出\n",
        "ans = torch.rand(5,3)\n",
        "torch.add(x,y,out=ans)\n",
        "print(ans)\n",
        "# 形式四 inplace 将一个加数加到另一个加数上，相当于  a=a+b\n",
        "y.add_(x)    # 相当于y = x+y\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0805,  1.5663,  0.5424],\n",
            "        [ 0.6752,  3.3876, -0.8825],\n",
            "        [ 1.4824,  1.2662, -0.0783],\n",
            "        [ 0.1228, -0.9484,  0.4187],\n",
            "        [-0.7978,  0.8151,  0.6267]])\n",
            "tensor([[-0.0805,  1.5663,  0.5424],\n",
            "        [ 0.6752,  3.3876, -0.8825],\n",
            "        [ 1.4824,  1.2662, -0.0783],\n",
            "        [ 0.1228, -0.9484,  0.4187],\n",
            "        [-0.7978,  0.8151,  0.6267]])\n",
            "tensor([[-0.0805,  1.5663,  0.5424],\n",
            "        [ 0.6752,  3.3876, -0.8825],\n",
            "        [ 1.4824,  1.2662, -0.0783],\n",
            "        [ 0.1228, -0.9484,  0.4187],\n",
            "        [-0.7978,  0.8151,  0.6267]])\n",
            "tensor([[-0.0805,  1.5663,  0.5424],\n",
            "        [ 0.6752,  3.3876, -0.8825],\n",
            "        [ 1.4824,  1.2662, -0.0783],\n",
            "        [ 0.1228, -0.9484,  0.4187],\n",
            "        [-0.7978,  0.8151,  0.6267]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG8Hh13KtcD2",
        "colab_type": "code",
        "outputId": "ceef65b3-1214-4a08-85df-af5a54661632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# 索引  我们可以使用类似numpy的索引操作来访问tensor中的部分，需注意，索引出来的结果与原数据 共享内存，一个改了另一个也就改了\n",
        "y = x[0, :]\n",
        "print(y)\n",
        "print(x[0, :])\n",
        "y += 1    # 这里的加1是张量加1，和numpy是一样的，广播机制\n",
        "print(y)\n",
        "print(x[0, :])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.4240, 1.2552, 1.5928])\n",
            "tensor([1.4240, 1.2552, 1.5928])\n",
            "tensor([2.4240, 2.2552, 2.5928])\n",
            "tensor([2.4240, 2.2552, 2.5928])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x62obR3MuKdx",
        "colab_type": "code",
        "outputId": "47a8594e-59fe-404f-982d-bb4eb226fe7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# 除了常用的索引选择数据之外，pytorch还提供了一些高级的选择函数\n",
        "# index_select(input, dim, index)  在指定维度dim上选取，比如选取某些行，某些列\n",
        "# y = torch.index_select(x, 1, 2)    这里报错了，index参数需要输入 tensor \n",
        "print(x)\n",
        "y = torch.index_select(x,1,torch.tensor([1]))  \n",
        "print(y)\n",
        "\n",
        "# masked_select(input, mask)  该函数找出input中关于mask选择条件的元素，并返回一维tensor\n",
        "x = torch.randn(4,3)\n",
        "print(x)\n",
        "mask = (x > 0)    # 原先写成 mask = torch.ByteTensor(x>0)已经不对了\n",
        "print(mask)\n",
        "\n",
        "ans = torch.masked_select(x,mask)\n",
        "print(ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.1511,  0.7252, -0.7877],\n",
            "        [-0.4114,  0.3004, -1.5361],\n",
            "        [ 0.3173,  0.1144, -1.5722],\n",
            "        [-0.3602, -1.1143,  0.2565]])\n",
            "tensor([[ 0.7252],\n",
            "        [ 0.3004],\n",
            "        [ 0.1144],\n",
            "        [-1.1143]])\n",
            "tensor([[ 1.4449, -1.7046, -0.6048],\n",
            "        [ 0.3801,  0.9479, -0.8370],\n",
            "        [ 0.5389,  1.0403, -0.6453],\n",
            "        [ 2.2299, -0.1705,  1.4203]])\n",
            "tensor([[ True, False, False],\n",
            "        [ True,  True, False],\n",
            "        [ True,  True, False],\n",
            "        [ True, False,  True]])\n",
            "tensor([1.4449, 0.3801, 0.9479, 0.5389, 1.0403, 2.2299, 1.4203])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DqpZoMvvbdQ",
        "colab_type": "code",
        "outputId": "3ead09cd-cadd-4b11-faf5-8759e167e18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# 改变形状  用view()来改变tensor的形状\n",
        "y = x.view(12)\n",
        "z = x.view(-1,4)   # -1可以根据另外一个维度算出来，相当于由后面的值来确定他\n",
        "print(x.size(), y.size(), z.size())\n",
        "# 注意 view()返回的新的tensor还是和上面的索引取数是一样的，还是共享内存，通过任何一个tensor改变值，最后都会改变，view顾名思义就是改变了原有\n",
        "# tensor的观察角度 如果我们像返回一个 真正的新的副本即不共享内存，该怎么办。\n",
        "# pytorch还提供了一个reshape()来改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用，建议先clone()创造一个副本，然后再使用view()\n",
        "print(x)\n",
        "x_cp = x.clone().view(12)\n",
        "print(x_cp)\n",
        "x_cp[2] = 0.3\n",
        "print(x)\n",
        "print(x_cp)\n",
        "# 使用clone()还有一个好处，会被记录在计算图中，即梯度回传到副本时也会回传到源tensor。  这句话有待后面的了解"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3]) torch.Size([12]) torch.Size([3, 4])\n",
            "tensor([[ 1.4449, -1.7046, -0.6048],\n",
            "        [ 0.3801,  0.9479, -0.8370],\n",
            "        [ 0.5389,  1.0403, -0.6453],\n",
            "        [ 2.2299, -0.1705,  1.4203]])\n",
            "tensor([ 1.4449, -1.7046, -0.6048,  0.3801,  0.9479, -0.8370,  0.5389,  1.0403,\n",
            "        -0.6453,  2.2299, -0.1705,  1.4203])\n",
            "tensor([[ 1.4449, -1.7046, -0.6048],\n",
            "        [ 0.3801,  0.9479, -0.8370],\n",
            "        [ 0.5389,  1.0403, -0.6453],\n",
            "        [ 2.2299, -0.1705,  1.4203]])\n",
            "tensor([ 1.4449, -1.7046,  0.3000,  0.3801,  0.9479, -0.8370,  0.5389,  1.0403,\n",
            "        -0.6453,  2.2299, -0.1705,  1.4203])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uztjUUhU3MuK",
        "colab_type": "code",
        "outputId": "1b395700-a45f-4918-82ff-53c9d4e6d5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 另外一个常用函数就是item()，它可以将一个标量tensor 转换成一个python number\n",
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item())\n",
        "# 标量tensor也就是包含一个元素的tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-2.5068])\n",
            "-2.5067639350891113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ3G3nqJ3Z5W",
        "colab_type": "code",
        "outputId": "4091d10e-e7a8-43fa-b151-7812c32d4dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# 线性代数\n",
        "# pytorch还支持一些线性函数，这里提一下。\n",
        "# t->转置    dot->内积  cross->外积   inverse->求逆矩阵   trace->对角线元素之和（矩阵的迹)\n",
        "#  diag->对角线元素   triu/tril->矩阵的上三角/下三角   等等很多，如果用得到可以自定查看文档\n",
        "x = torch.randn(4,3)\n",
        "print(x)\n",
        "print(torch.t(x))\n",
        "# print(torch.inverse(x))    # 逆矩阵必须是n*n的\n",
        "# print(torch.cross(x,torch.t(x)))   这里报错的原因是因为 内积外积都是针对向量的而不是二维或者更高维的\n",
        "print(torch.mm(x,torch.t(x)))   # 矩阵乘法用 torch.mm（）"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-2.2794,  0.4233, -0.6875],\n",
            "        [ 0.1768,  1.9702,  1.9058],\n",
            "        [-0.0081, -0.3763,  1.9659],\n",
            "        [ 2.0266, -0.5763, -1.1529]])\n",
            "tensor([[-2.2794,  0.1768, -0.0081,  2.0266],\n",
            "        [ 0.4233,  1.9702, -0.3763, -0.5763],\n",
            "        [-0.6875,  1.9058,  1.9659, -1.1529]])\n",
            "tensor([[ 5.8474, -0.8791, -1.4924, -4.0707],\n",
            "        [-0.8791,  7.5450,  3.0037, -2.9742],\n",
            "        [-1.4924,  3.0037,  4.0063, -2.0659],\n",
            "        [-4.0707, -2.9742, -2.0659,  5.7682]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF__4kY3fX-t",
        "colab_type": "code",
        "outputId": "3ae4244c-89bc-4456-9462-85e399f69d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# 2.2.3 广播机制\n",
        "# 前面我们看到如何对两个形状相同的tensor元素做按元素运算。当对两个形状不同的tensor按元素运算时，可能会触发广播机制：先适当复制元素使\n",
        "# 这两个tensor形状相同后再按元素运算。例如\n",
        "import torch\n",
        "x =  torch.arange(1,3).view(1,2)\n",
        "print(x)\n",
        "y = torch.arange(1,4).view(3,1)\n",
        "print(y)\n",
        "print(x+y)\n",
        "# 因为这两个tensor形状不一样，所以他们就都转换为3*2 形状的tensor，原来没有元素的地方就是直接复制元素就可以了"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2]])\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "tensor([[2, 3],\n",
            "        [3, 4],\n",
            "        [4, 5]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbP_r4PCg6Iy",
        "colab_type": "code",
        "outputId": "4eac1d26-de5f-4851-f14f-73d5fd6a5499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 2.2.4 运算的内存开销\n",
        "# 前面说了，索引、view是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用python自带的id函数：\n",
        "# 如果两个实例的ID一致，那么他们所对应的内存地址相同；反之则不同\n",
        "x = torch.tensor([1,2])\n",
        "y = torch.tensor([4,5])\n",
        "id_origin = id(y)\n",
        "y = y+x\n",
        "id_now = id(y)\n",
        "print(id_origin, id_now)\n",
        "# 结果是前后两个y并不一样啊，说明赋值之后又重开了新内存"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "139820827847848 139820827847632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HuWKcyqh6Pw",
        "colab_type": "code",
        "outputId": "ac269c90-baf5-4cce-a65d-8f1aef88a523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 如果我们想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。下面我们把 x+y 的结果通过[:]写进y对应的内存中\n",
        "x = torch.tensor([1,2])\n",
        "y = torch.tensor([2,4])\n",
        "id_before = id(y)\n",
        "y[:] = y+x\n",
        "print(id(y)==id_before)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqCIUFX5jciA",
        "colab_type": "code",
        "outputId": "15d85a6c-01e9-4f42-c75f-d3a5aa751e2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 我们还可以使用运算符全名函数中的out参数或者自加运算符+= （也就是add_()）达到上述效果，\n",
        "x = torch.tensor([2,3])\n",
        "y = torch.tensor([1,1])\n",
        "z = torch.tensor([4,3])\n",
        "id_y_before = id(y)\n",
        "id_z_before = id(z)\n",
        "torch.add(x,y,out=y)\n",
        "z+=x   # 这个等于 z.add_(x)\n",
        "print(\"y:\"+str(id(y)==id_y_before))\n",
        "print(\"z:\"+str(id(z)==id_z_before))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y:True\n",
            "z:True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIBjlxSAlAnw",
        "colab_type": "code",
        "outputId": "4ece8beb-f294-4871-9d5c-a7e04862fee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# 2.2.5 tensor 和  numpy相互转换\n",
        "# 我们很容易用  numpy()  和 from_numpy()将 tensor 和 numpy中的数字相互转换\n",
        "# 但是需要注意一点：这两个函数所产生的tensor和 numpy中的数组共享相同的内存（所以转换很快)\n",
        "\n",
        "# tensor转numpy  -->使用numpy() 将tensor转换成numpy数组\n",
        "a = torch.ones(5)\n",
        "b = a.numpy()\n",
        "print(a,b)\n",
        "a += 1\n",
        "print(a,b)\n",
        "b +=1\n",
        "print(a,b)    #共享内存"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
            "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
            "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgARl6IBmS5N",
        "colab_type": "code",
        "outputId": "30acedf9-79b7-4163-e3a6-121287cb615b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# numpy数组转tensor   from_numpy() 将numpy数组转换为 tensor\n",
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a)\n",
        "print(a,b)\n",
        "\n",
        "a +=1 \n",
        "print(a,b)\n",
        "\n",
        "b+=1\n",
        "print(a,b)    # 共享内存\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qkB7ZX0nNd8",
        "colab_type": "code",
        "outputId": "abc75b52-6b76-4b9a-add7-e2f7e9af2870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 还有一个常用方法直接用 torch.tensor()将numpy数组转换成 tensor，需要注意的是该方法总是会进行数据拷贝，返回的tensor和原来数据不进行内存共享\n",
        "c = torch.tensor(a)\n",
        "a +=1\n",
        "print(a,c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7nIjlM3nxGV",
        "colab_type": "code",
        "outputId": "5c9c21fd-b761-4b9d-dd23-5b80250fa6e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# 2.2.6 tensor on GPU\n",
        "# 用方法to() 可以将 tensor 在CPU 和GPU 之间相互移动\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  y = torch.ones_like(x, device=device)\n",
        "  x = x.to(device)\n",
        "  z = x+y\n",
        "  print(z)\n",
        "  print(z.to(\"cpu\",torch.double))   # to() 还可以同时改变数据类型   \n",
        "  # 但是这里移动之后，z还是在GPU上面，必须再给他赋给某一个变量，这个变量才是在cpu的内存中了\n",
        "  c = z.to(\"cpu\",torch.double)\n",
        "  print(c)\n",
        "  print(z.device)\n",
        "  print(z.to(\"cpu\",torch.double).device)\n",
        "  print(c.device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 4], device='cuda:0')\n",
            "tensor([3., 4.], dtype=torch.float64)\n",
            "tensor([3., 4.], dtype=torch.float64)\n",
            "cuda:0\n",
            "cpu\n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}