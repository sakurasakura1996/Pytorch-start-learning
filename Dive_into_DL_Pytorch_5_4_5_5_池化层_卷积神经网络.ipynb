{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Dive-into-DL-Pytorch-5.4--5.5-池化层-卷积神经网络.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakurasakura1996/Pytorch-start-learning/blob/master/Dive_into_DL_Pytorch_5_4_5_5_%E6%B1%A0%E5%8C%96%E5%B1%82_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etU-X0b8UkJD",
        "colab_type": "text"
      },
      "source": [
        "回忆一下，5.1节 二维卷积层里介绍的图像物体边缘检测应用中，我们构造卷积核从而精确地找到了像素变化的位置。在输入经过卷积核之后，导致相邻元素不一样，这就意味着物体边缘通过这两个元素之间。但实际图像中，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后面的模式识别造成不便。\n",
        "所以我们本节提出的池化层pooling层，他的提出就是为了缓解卷积层对位置的过度敏感性"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz19GA5_UkJI",
        "colab_type": "text"
      },
      "source": [
        "5.4.1 二维最大池化层   和   平均池化层\n",
        "同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yva6fjGUUkJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为2*2最大池化的输入。设卷积层输入X，池化层输出Y。在加入一个最大池化层之后，\n",
        "# 无论 X[i,j] 和 X[i,j+1]值不同，还是X[i,j+1] 和 X[i,j+2]不同，池化层输出均有Y[i,j] =1，也就是说，使用2x2最大池化层时，只要卷积层识别的模式在\n",
        "# 高和宽上移动不超过一个元素，我们依然可以将他检测出来。  下面实现池化层的前向计算实现\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def pool2d(X, pool_size,mode = 'max'):\n",
        "    X = X.float()\n",
        "    p_h, p_w = pool_size\n",
        "    Y = torch.zeros(X.shape[0]- p_h +1,X.shape[1]-p_w+1)\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            if mode == 'max':\n",
        "                Y[i,j] = X[i:i+p_h,j:j+p_w].max()\n",
        "            elif mode == 'avg':\n",
        "                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()\n",
        "    return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4DFHB7UkJP",
        "colab_type": "code",
        "outputId": "df133d3f-01ae-4fe6-fb51-276695ad238d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "X = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n",
        "print(pool2d(X,(2,2)))\n",
        "print(pool2d(X,(2,2),'avg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 5.],\n",
            "        [7., 8.]])\n",
            "tensor([[2., 3.],\n",
            "        [5., 6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NKp9t2CUkJZ",
        "colab_type": "code",
        "outputId": "be6813a1-9e5c-4a1a-9551-4bedaf9df2f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# 池化层填充和步幅\n",
        "# 同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层机制一样的，我们通过nn模块中的二维\n",
        "# 最大池化层Maxpool2d来演示池化层填充和步幅的工作机制。我们先构造一个形状为（1，1，4，4）的输入数据，前面两个维度分别是批量和通道\n",
        "X = torch.arange(16,dtype=torch.float).view(1,1,4,4)\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.,  1.,  2.,  3.],\n",
            "          [ 4.,  5.,  6.,  7.],\n",
            "          [ 8.,  9., 10., 11.],\n",
            "          [12., 13., 14., 15.]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMIW6kvUkJg",
        "colab_type": "code",
        "outputId": "4eab773a-e12b-4fc5-a908-bd986ca5b404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 默认情况下，Maxpool2d实例里步幅和池化窗口形状不同。下面使用形状为（3，3）的池化窗口。默认获得形状为（3，3）的步幅\n",
        "pool2d = nn.MaxPool2d(3)\n",
        "pool2d(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[10.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmTRQEhEUkJq",
        "colab_type": "code",
        "outputId": "a3457363-3e07-41d6-daed-9f6dcf4d9656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# 我们可以手动指定步幅和填充\n",
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "print(pool2d(X))\n",
        "print(pool2d(X).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 5.,  7.],\n",
            "          [13., 15.]]]])\n",
            "torch.Size([1, 1, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1mdH55GUkJv",
        "colab_type": "code",
        "outputId": "1a9e2e5b-724a-486b-b471-87e1fb94a70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "pool2d = nn.MaxPool2d((2,4), padding=(1,2),stride=(2,3))\n",
        "pool2d(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1.,  3.],\n",
              "          [ 9., 11.],\n",
              "          [13., 15.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sP_ZQhRUkJ6",
        "colab_type": "code",
        "outputId": "aebe5fff-d42c-4435-c40c-0a59ad23cb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# 5.4.3多通道\n",
        "# 在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。下面\n",
        "# 将数组X 和 X+1 在通道维上连接来构造通道数为2的输入。\n",
        "X = torch.cat((X,X+1),dim=1)\n",
        "print(X)\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.,  1.,  2.,  3.],\n",
            "          [ 4.,  5.,  6.,  7.],\n",
            "          [ 8.,  9., 10., 11.],\n",
            "          [12., 13., 14., 15.]],\n",
            "\n",
            "         [[ 1.,  2.,  3.,  4.],\n",
            "          [ 5.,  6.,  7.,  8.],\n",
            "          [ 9., 10., 11., 12.],\n",
            "          [13., 14., 15., 16.]]]])\n",
            "torch.Size([1, 2, 4, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW1CynGRUkJ_",
        "colab_type": "code",
        "outputId": "31454599-5fd5-4398-b7bb-0b7942f23ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# 池化后，我们发现输出通道仍然是2\n",
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "print(pool2d(X))\n",
        "print(pool2d(X).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 5.,  7.],\n",
            "          [13., 15.]],\n",
            "\n",
            "         [[ 6.,  8.],\n",
            "          [14., 16.]]]])\n",
            "torch.Size([1, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOm1tzPUkKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conclusion\n",
        "# 池化层的一个主要作用是缓解卷积层对位置的过渡敏感性。我们可以指定池化层的填充  和  步幅，  池化层的输出通道数和输入通道数相同，他不像 1x1 卷积核，可以改变通道数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R2XB4HCUkKN",
        "colab_type": "text"
      },
      "source": [
        "# 5.5 卷积神经网络\n",
        "在3.9节（多层感知机从零开始实现）里我们构造了一个含单隐藏层的多层感知机模型来对Fashion-MNIST数据集中的图像进行分类。每张图像高和宽均为28像素\n",
        "我们将图像中的像素逐行展开，得到784的向量，并输入进全连接层中。然而，这种分类方法有一定局限性\n",
        "1.图像在同一列邻近的像素在这个向量中可能相距较远。他们构成的模式可能难以被模型识别\n",
        "2.对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为1000像素的彩色照片（3通道）。即使全连接层输出格式仍是256，该层权重参数的形状是 3000000x256:它占用了大约3GB 的内存或显存。这带来过复杂的模型和过高的存储开销\n",
        "卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽的两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n",
        "## 5.5.1 LENET模型\n",
        "![image.png](attachment:image.png)\n",
        "LeNet分为卷积层块和全连接层块两个部分。\n",
        "卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5x5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数增加至16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为2x2，且步幅为2.由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠\n",
        "卷积层块的输出形状为（批量大小，通道，高，宽）。当卷积层快的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层快含3个全连接层。他们的输出个数分别是120，84和10，其中10为输出的类别个数。\n",
        "下面通过Sequential类来实现LeNet模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhPqf2ljUkKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torch import optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet,self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1,6,5), # in_channels,out_channels,kernel_size\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(2,2),  # kernel_size, stride\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16*4*4, 120),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(84, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, img):\n",
        "        feature = self.conv(img)\n",
        "        output = self.fc(feature.view(img.shape[0],-1))\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ0M59ToUkKX",
        "colab_type": "code",
        "outputId": "c029bb19-aefd-4585-9de2-7727daf8008a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "net = LeNet()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Sigmoid()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): Sigmoid()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwzw5oPFUkKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将\n",
        "# 高和宽分别减小为4，而池化层则将高和宽减半，但通道数则从1增加到16.全连接层则逐层减少输出个数\n",
        "# 直到变成图像的类别数为10\n",
        "# 5.5.2 获取数据和训练模型，人使用Fashion-MNIST数据集，然后用LeNet模型"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Ro1p9aVFhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms  as transforms\n",
        "import sys\n",
        "\n",
        "# 加载MNIST数据\n",
        "def load_data_fashion_mnist(batch_size):\n",
        "  mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST',train=True,transform=transforms.ToTensor(),download=True)\n",
        "  mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST',train=False,transform=transforms.ToTensor(),download=True)\n",
        "  if sys.platform.startswith('win'):\n",
        "    num_workers = 0 \n",
        "  else:\n",
        "    num_workers = 4\n",
        "  train_iter = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  test_iter = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "  return train_iter,test_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLTqK3EEZcRI",
        "colab_type": "code",
        "outputId": "5b7a00af-6346-4253-90a9-4fa432182fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "print(type(train_iter))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ozl9eSZjxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 使用GPU来运算，下面来写evaluate_accuracy函数做修改。支持GPU\n",
        "def evaluate_accuracy(data_iter, net, device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "  acc_sum, n =0.0, 0\n",
        "  with torch.no_grad():\n",
        "    for X,y in data_iter:\n",
        "      if isinstance(net, torch.nn.Module):\n",
        "        net.eval()   # 评估模式，这回关闭dropout\n",
        "        acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
        "        net.train()  # 改回训练模式\n",
        "      else:\n",
        "        if('is_training' in net.__code__.co_varnames):\n",
        "          acc_sum += (net(X,is_training=False).argmax(dim=1) == y).float().sum().item()\n",
        "        else:\n",
        "          acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
        "      n += y.shape[0]\n",
        "  return acc_sum / n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCbPeA0NegdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
        "  net = net.to(device)\n",
        "  print(\"training on \",device)\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  batch_count = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "    for X,y in train_iter:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_hat = net(X)\n",
        "      l = loss(y_hat,y)\n",
        "      optimizer.zero_grad()\n",
        "      l.backward()\n",
        "      optimizer.step()\n",
        "      train_l_sum += l.cpu().item()\n",
        "      train_acc_sum +=(y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
        "      n += y.shape[0]\n",
        "      batch_count +=1\n",
        "    test_acc = evaluate_accuracy(test_iter, net)\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f,time %.1f sec'% (epoch + 1, train_l_sum / batch_count,train_acc_sum / n, test_acc, time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9xasYv8ge5v",
        "colab_type": "code",
        "outputId": "e081a54b-ba2e-422a-e31b-a506fd9908db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "lr, num_epochs = 0.001, 10\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on  cuda\n",
            "epoch 1, loss 0.5543, train acc 0.783, test acc 0.780,time 5.7 sec\n",
            "epoch 2, loss 0.2655, train acc 0.793, test acc 0.788,time 5.7 sec\n",
            "epoch 3, loss 0.1701, train acc 0.801, test acc 0.796,time 5.6 sec\n",
            "epoch 4, loss 0.1227, train acc 0.809, test acc 0.803,time 5.7 sec\n",
            "epoch 5, loss 0.0950, train acc 0.817, test acc 0.812,time 5.8 sec\n",
            "epoch 6, loss 0.0769, train acc 0.822, test acc 0.818,time 5.8 sec\n",
            "epoch 7, loss 0.0641, train acc 0.829, test acc 0.823,time 5.6 sec\n",
            "epoch 8, loss 0.0545, train acc 0.835, test acc 0.825,time 5.6 sec\n",
            "epoch 9, loss 0.0475, train acc 0.839, test acc 0.828,time 5.7 sec\n",
            "epoch 10, loss 0.0417, train acc 0.844, test acc 0.837,time 5.5 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeOBGLRDhcjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}