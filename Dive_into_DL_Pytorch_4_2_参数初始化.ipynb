{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Dive-into-DL-Pytorch-4.2-参数初始化.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakurasakura1996/Pytorch-start-learning/blob/master/Dive_into_DL_Pytorch_4_2_%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk6IjKGRWXWS",
        "colab_type": "code",
        "colab": {},
        "outputId": "e279e50b-5ea3-4993-a702-31b56ef3f542"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(4,3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(3, 1)   # 就这样定义 pytorch已经进行默认初始化了\n",
        ")\n",
        "print(net)\n",
        "X = torch.rand(2, 4)\n",
        "Y = net(X).sum()   # 记得一定要在这里加上sum() 也就是返回的结果一定要为标量\n",
        "# 不然后面反向传播时 Y.backward()就会报错了baby\n",
        "print(Y)\n",
        "print(Y.sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            ")\n",
            "tensor(0.0336, grad_fn=<SumBackward0>)\n",
            "tensor(0.0336, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMZyD3ORWXWZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "85685195-4301-426b-9da6-8bcf35d08177"
      },
      "source": [
        "# 访问模型参数\n",
        "# 回忆Sequential类和Module类的继承关系，对于Sequential实例中含模型参数的层，我们可以通过\n",
        "# Module类的parameters() or  named_parameters()方法来访问到所有的参数，以迭代器的形式放回。\n",
        "# 后者除了返回参数Tensor外还会返回其名字。下面访问多层敢直接net的所有参数\n",
        "print(type(net))\n",
        "print(type(net.named_parameters()))\n",
        "for name, param in net.named_parameters():\n",
        "    print(name, param.size())\n",
        "\n",
        "# 可见返回的名字自动加上了层数的索引作为前缀。"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "<class 'generator'>\n",
            "0.weight torch.Size([3, 4])\n",
            "0.bias torch.Size([3])\n",
            "2.weight torch.Size([1, 3])\n",
            "2.bias torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxzlpjpbWXWd",
        "colab_type": "code",
        "colab": {},
        "outputId": "e2d36347-4c83-479d-d1cc-516265d73f5e"
      },
      "source": [
        "# 我们再来访问net中单层的参数，对于使用Sequential 类构造的神经网络，我们可以通过方括号【】\n",
        "# 来访问网络的任一层，索引0表示隐藏层为Sequential实例最先添加的层\n",
        "for name, param in net[0].named_parameters():\n",
        "    print(name, param.size(), type(param))\n",
        "# parameters是param的类型，其实就是tensor的子类，和Tensor不同的是如果一个Tensor是\n",
        "# Parameter类的话，那么它会自动被添加到模型的参数列表里，来看下麦呢这个例子"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
            "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "223h-A1MWXWi",
        "colab_type": "code",
        "colab": {},
        "outputId": "80449404-6f35-41da-9f28-1abc9c292815"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "        self.weight1 = nn.Parameter(torch.rand(20, 20))\n",
        "        self.weight2 = torch.rand(20,20)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        pass\n",
        "\n",
        "n = MyModel()\n",
        "for name, param in n.named_parameters():\n",
        "    print(name,param.size(),type(param))\n",
        "    \n",
        "# 这个例子就是说明 如果一个tensor是Paramter类的实例的话，它会自动加到 参数列表中去"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight1 torch.Size([20, 20]) <class 'torch.nn.parameter.Parameter'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYJyBwy9WXWt",
        "colab_type": "code",
        "colab": {},
        "outputId": "acc5dea0-6fa0-433a-977b-6e22942bac1b"
      },
      "source": [
        "# 因为Parameter是Tensor，即tensor拥有的属性它都有，比如可以根据data来访问参数数值\n",
        "# 用grad 来访问参数梯度\n",
        "weight_0 = list(net[0].parameters())[0]\n",
        "print(weight_0.data)\n",
        "print(weight_0.grad)   # 反向传播前梯度为None\n",
        "Y.backward()\n",
        "print(weight_0.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1987, -0.1055,  0.1257, -0.0772],\n",
            "        [ 0.2914,  0.4856,  0.4213,  0.3935],\n",
            "        [ 0.4958,  0.1483,  0.4524, -0.4767]])\n",
            "None\n",
            "tensor([[0.1094, 0.2646, 0.2896, 0.2278],\n",
            "        [0.2249, 0.5438, 0.5952, 0.4682],\n",
            "        [0.0751, 0.1816, 0.1988, 0.1564]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckbwhwjCWXWz",
        "colab_type": "code",
        "colab": {},
        "outputId": "0b3be564-d8d0-4863-9483-76fe094d305f"
      },
      "source": [
        "# 初始化模型参数\n",
        "# 之前提到pytorch中 nn.Module的模块参数都采取了较为合理的初始化策略。不同类型的layer采用的哪一种初始化方法去看源码\n",
        "# 但我们经常需要用其他方法来初始化权重。pytorch中提供的init模块提供了多种预设的初始化方法。看下面例子\n",
        "for name, param in net.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        init.normal_(param, mean=0, std=0.01)\n",
        "        print(name, param.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.weight tensor([[-0.0069, -0.0031, -0.0157, -0.0024],\n",
            "        [-0.0140, -0.0005,  0.0058, -0.0035],\n",
            "        [ 0.0204,  0.0002, -0.0059,  0.0009]])\n",
            "2.weight tensor([[-0.0023,  0.0020, -0.0125]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPwv154qWXW5",
        "colab_type": "code",
        "colab": {},
        "outputId": "a3007740-76a0-444e-d644-857835471faa"
      },
      "source": [
        "# 下面使用常数来初始化权重参数\n",
        "for name, param in net.named_parameters():\n",
        "    if 'bias' in name:\n",
        "        init.constant_(param, val=0)\n",
        "        print(name, param.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.bias tensor([0., 0., 0.])\n",
            "2.bias tensor([0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "585lZv15WXW9",
        "colab_type": "code",
        "colab": {},
        "outputId": "af2d4dff-8ff6-47e0-c8af-41f78cb60f26"
      },
      "source": [
        "# 如果只想对某个特定参数进行初始化，我们可以调用Parameter类的initialize函数，它与\n",
        "# Block类提供的initialize函数使用方法一致，下例中我们对隐藏层的权重使用 Xavier随机初始化方法\n",
        "# 4.2.3 自定义初始化方法\n",
        "# 有时候初始化方法我们需要的 init模块中并没有提供，所以我们需要实现一个初始化方法，从而能够像使用其他初始化方法使用它，\n",
        "# 在此之前我们先看看pytorch 怎么实现这些初始化方法的，例如 torch.nn.init.normal_\n",
        "def normal_(tensor, mean=0, std=1):\n",
        "    with torch.no_grad():\n",
        "        return tensor.normal_(mean,std)\n",
        "# 可以看到这就是一个in-place改变tensor值的函数，而且此过程不记录梯度。类似我们就可以实现自己定义的初始化方法。\n",
        "# 下面例子中，我们令权重 有一半概率初始化为0，另一半概率初始化为[-10,-5]和[5,10]两个区间里均匀分布的随机数\n",
        "def init_weight_(tensor):\n",
        "    with torch.no_grad():\n",
        "        tensor.uniform_(-10,10)\n",
        "        tensor *= (tensor.abs() >= 5).float()\n",
        "        \n",
        "for name, param in net.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        init_weight_(param)\n",
        "        print(name, param.data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.weight tensor([[ 8.0981, -5.9484,  6.0992,  6.4422],\n",
            "        [-8.1373, -9.6199,  0.0000, -0.0000],\n",
            "        [ 0.0000, -0.0000,  9.6596,  0.0000]])\n",
            "2.weight tensor([[-8.4161,  9.9235, -9.2910]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uYRFWlvWXXB",
        "colab_type": "code",
        "colab": {},
        "outputId": "61ae04c6-24a8-4324-fce5-1a8354010ff3"
      },
      "source": [
        "# 此外，我们还可以通过改变这些参数的data来改写模型参数值同时不会影响梯度\n",
        "for name, param in net.named_parameters():\n",
        "    if 'bias' in name:\n",
        "        param.data += 1\n",
        "        print(name, param.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.bias tensor([1., 1., 1.])\n",
            "2.bias tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poIvCzcpWXXL",
        "colab_type": "code",
        "colab": {},
        "outputId": "0cdaea0d-d088-4c55-f79a-d0313ccc7aa2"
      },
      "source": [
        "# 4.2.4 共享模型参数\n",
        "# 有些情况下，我们希望多个层之间共享模型参数。4.1.3提到了 Module类的forward函数里多次调用同一个层，此外如果我们传入\n",
        "# Sequential的模块是同一个Module实例的话参数也是共享的\n",
        "linear = nn.Linear(1, 1, bias=False)\n",
        "net = nn.Sequential(linear, linear)\n",
        "print(net)\n",
        "for name, param in net.named_parameters():\n",
        "    init.constant_(param, val=3)\n",
        "    print(name, param.data)\n",
        "# Sequential网络中有两个linear层，都是同一个实例，所以他们的参数是共享的，输出参数列表时也只有一个\n",
        "# 在内存中，这两个线性层其实也是一个对象\n",
        "print(id(net[0]) == id(net[1]))\n",
        "print(id(net[0].weight) == id(net[1].weight))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
            "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
            ")\n",
            "0.weight tensor([[3.]])\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfEscK6LWXXQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "9bd62f41-ac9e-4a79-ff8f-f595b4f4f744"
      },
      "source": [
        "# 因为模型参数里面包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的\n",
        "x = torch.ones(1,1)\n",
        "y = net(x).sum()\n",
        "print(x, y)\n",
        "y.backward()\n",
        "print(net[0].weight.grad)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.]]) tensor(9., grad_fn=<SumBackward0>)\n",
            "tensor([[6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-qM0y38WXXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}