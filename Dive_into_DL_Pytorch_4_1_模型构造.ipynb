{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Dive-into-DL-Pytorch-4.1-模型构造.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakurasakura1996/Pytorch-start-learning/blob/master/Dive_into_DL_Pytorch_4_1_%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4fvPR9lWW4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy0ff_cUWW4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MLP,self).__init__(**kwargs)\n",
        "        self.hidden = nn.Linear(784, 256)   # 隐藏层\n",
        "        self.act = nn.ReLU()\n",
        "        self.output = nn.Linear(256, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.act(self.hidden(x))\n",
        "        return self.output(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPYKshniWW4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.rand(1, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYkMPd3NWW4f",
        "colab_type": "code",
        "colab": {},
        "outputId": "b5e8f9bd-c243-400e-9f3d-3ed60f283462"
      },
      "source": [
        "net = MLP()\n",
        "print(net)\n",
        "net(X)\n",
        "# 这里并没有将 Module类命名为 Layer层  或者 Model模型这类的名字，是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如pytorh提供\n",
        "# 的Linear类），又可以是一个模型（如这里定义的MLP类）或者是模型的一部分。下面两个例子来展示他的灵活性"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (act): ReLU()\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0313,  0.1702, -0.0252,  0.0855, -0.1259, -0.0680, -0.0421,  0.0860,\n",
              "         -0.1692,  0.0009]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSlitRrKWW4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 有很多继承自Module的类  如 Sequential类，  ModuleList，   ModuleDict\n",
        "# 当模型的前向计算为简单串联各个层的计算时，Sequential类可以通过简单的方式定义模型。它可以接受一个子模块的有序字典（OrderDict)或者一系列子模块\n",
        "# 作为参数来逐一添加 Module实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。下面实现一个与Sequential相同功能的MySequential类。加深理解\n",
        "class MySequential(nn.Module):\n",
        "    from collections import OrderedDict\n",
        "    def __init__(self, *args):\n",
        "        super(MySequential, self).__init__()\n",
        "        if len(args)==1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderDict\n",
        "            for key, module in args[0].items():\n",
        "                self.add_module(key,module)  # add_module方法将会module添加进self._modules(一个OrderDict)\n",
        "        else:\n",
        "            for idx, module in enumerate(args):\n",
        "                self.add_module(str(idx), module)\n",
        "                \n",
        "    def forward(self, input):\n",
        "        # self._modules返回一个 OrderDict,保证会按照成员添加时的顺序遍历\n",
        "        for module in self._modules.values():\n",
        "            input = module(input)\n",
        "        return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClSP4DBJWW40",
        "colab_type": "code",
        "colab": {},
        "outputId": "fabfbd6f-1629-4a79-8f4d-0e8edf553f01"
      },
      "source": [
        "net = MySequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 10),\n",
        ")\n",
        "print(net)\n",
        "net(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MySequential(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0575,  0.0974,  0.0011, -0.1859,  0.0505,  0.1051, -0.1154,  0.0543,\n",
              "         -0.2051,  0.1766]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecQ2Ibn6WW4-",
        "colab_type": "code",
        "colab": {},
        "outputId": "66e57202-5fb7-4283-e76c-b742f0967731"
      },
      "source": [
        "# ModuleList类\n",
        "# ModuleList接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作\n",
        "import torch.nn as nn\n",
        "net = nn.ModuleList([nn.Linear(784, 256),nn.ReLU()])\n",
        "net.append(nn.Linear(256, 10))\n",
        "print(net[-1])\n",
        "print(net)\n",
        "# net(X)  这里 不能直接调用net(x)因为 ModuleList 类似于list,模型中未自动实现forward函数"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleList(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoukjRhUWW5E",
        "colab_type": "code",
        "colab": {},
        "outputId": "dc0f324f-6cc0-4d46-ffb8-19843051dca9"
      },
      "source": [
        "for model in net:\n",
        "    X = model(X)\n",
        "print(X)    # 这种表示法可以实现简单的前向计算"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1012,  0.1225, -0.1777,  0.0598, -0.0754,  0.1292,  0.0904,  0.0635,\n",
            "         -0.0614,  0.1253]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxLyvRFWW5N",
        "colab_type": "code",
        "colab": {},
        "outputId": "1a8cad46-2c9f-4b46-d24f-80750231dee2"
      },
      "source": [
        "# ModuleDict类 接受一个子模块的字典作为输入，然后也可以类似字典那样进行添加访问操作\n",
        "net = nn.ModuleDict({\n",
        "    'linear': nn.Linear(784, 256),\n",
        "    'act': nn.ReLU(),\n",
        "})\n",
        "net['output'] = nn.Linear(256, 10)\n",
        "print(net['linear'])\n",
        "print(net.output)\n",
        "print(net)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=256, bias=True)\n",
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleDict(\n",
            "  (act): ReLU()\n",
            "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZE4oHuLWW5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构造复杂的模型\n",
        "# 虽然上面介绍的这些类可以使模型构造更加简单，且不需要定义forward函数但直接继承Module类可以极大地扩展模型构造的灵活性。下面我们构造一个稍微\n",
        "# 复杂的网络 FancyMLP。这个网络中，我们通过get_constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常熟参数外，我们\n",
        "# 还使用Tensor的函数和Python的控制流，并多次调用相同的层\n",
        "class FancyMLP(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FancyMLP,self).__init__(**kwargs)\n",
        "        \n",
        "        self.rand_weight = torch.rand((20,20),requires_grad = False)   # 不可训练参数（常数参数）\n",
        "        self.linear = nn.Linear(20, 20)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        # 使用创建的常数参数,以及nn.functional中的relu函数和mm函数\n",
        "        x = nn.functional.relu(torch.mm(x,self.rand_weight.data)+1)\n",
        "        # 复用全连接层,等价于 两个全连接层 共享参数\n",
        "        x = self.linear(x)\n",
        "        # 控制流，我们这里需要调用item函数来返回标量进行比较\n",
        "        while x.norm().item() > 1:\n",
        "            x /= 2\n",
        "        if x.norm().item() <0.8:\n",
        "            x *= 10\n",
        "        return x.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl1x6eHIWW5W",
        "colab_type": "code",
        "colab": {},
        "outputId": "2e5ffd31-ddc2-4ce5-ccfd-b9fcadefe84e"
      },
      "source": [
        "X = torch.rand(2, 20)\n",
        "net = FancyMLP()\n",
        "print(net)\n",
        "net(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FancyMLP(\n",
            "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-11.0381, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjng-fFQWW5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}